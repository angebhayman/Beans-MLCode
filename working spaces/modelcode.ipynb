{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>Q9</th>\n",
       "      <th>Q10</th>\n",
       "      <th>Q11</th>\n",
       "      <th>Q12</th>\n",
       "      <th>Q13</th>\n",
       "      <th>Q14</th>\n",
       "      <th>Q15</th>\n",
       "      <th>Q16</th>\n",
       "      <th>Q17</th>\n",
       "      <th>Q18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.833333</td>\n",
       "      <td>2017</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>87.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>92.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>2017</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   average_rating  year    Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9  \\\n",
       "0        6.833333  2017  94.0  94.0  98.0  95.0  97.0  97.0  95.0  94.0  98.0   \n",
       "1        8.000000  2017  87.0  90.0  97.0  92.0  94.0  96.0  94.0  95.0  92.0   \n",
       "2        6.000000  2017  92.0  96.0  99.0  98.0  97.0  98.0  99.0  98.0  96.0   \n",
       "3        6.800000  2017  98.0  99.0  98.0  98.0  98.0  98.0  98.0  96.0  98.0   \n",
       "4        7.000000  2017  90.0  90.0  95.0  91.0  95.0  94.0  89.0  90.0  91.0   \n",
       "\n",
       "    Q10   Q11   Q12   Q13   Q14   Q15   Q16   Q17   Q18  \n",
       "0  96.0  96.0  96.0  96.0  98.0  94.0  94.0  96.0  89.0  \n",
       "1  92.0  89.0  97.0  95.0  91.0  94.0  95.0  94.0  88.0  \n",
       "2  97.0  96.0  98.0  98.0  98.0  98.0  96.0  98.0  89.0  \n",
       "3  97.0  99.0  97.0  99.0  99.0  99.0  96.0  97.0  91.0  \n",
       "4  90.0  90.0  89.0  90.0  94.0  92.0  91.0  96.0  90.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('../data/final_df_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>...</th>\n",
       "      <th>Q9_lag_1</th>\n",
       "      <th>Q10_lag_1</th>\n",
       "      <th>Q11_lag_1</th>\n",
       "      <th>Q12_lag_1</th>\n",
       "      <th>Q13_lag_1</th>\n",
       "      <th>Q14_lag_1</th>\n",
       "      <th>Q15_lag_1</th>\n",
       "      <th>Q16_lag_1</th>\n",
       "      <th>Q17_lag_1</th>\n",
       "      <th>Q18_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>87.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>92.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>2017</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>2017</td>\n",
       "      <td>93.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>91.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>7.375000</td>\n",
       "      <td>2022</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>7.478261</td>\n",
       "      <td>2022</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>6.967742</td>\n",
       "      <td>2022</td>\n",
       "      <td>86.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>6.992000</td>\n",
       "      <td>2022</td>\n",
       "      <td>82.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>7.601504</td>\n",
       "      <td>2022</td>\n",
       "      <td>89.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>85.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3921 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      average_rating  year     Q1    Q2     Q3    Q4     Q5     Q6     Q7  \\\n",
       "1           8.000000  2017   87.0  90.0   97.0  92.0   94.0   96.0   94.0   \n",
       "2           6.000000  2017   92.0  96.0   99.0  98.0   97.0   98.0   99.0   \n",
       "3           6.800000  2017   98.0  99.0   98.0  98.0   98.0   98.0   98.0   \n",
       "4           7.000000  2017   90.0  90.0   95.0  91.0   95.0   94.0   89.0   \n",
       "5           7.400000  2017   93.0  96.0   98.0  95.0   98.0   97.0   97.0   \n",
       "...              ...   ...    ...   ...    ...   ...    ...    ...    ...   \n",
       "3917        7.375000  2022  100.0  99.0  100.0  99.0  100.0  100.0  100.0   \n",
       "3918        7.478261  2022   96.0  94.0   98.0  93.0   98.0   93.0   93.0   \n",
       "3919        6.967742  2022   86.0  91.0   98.0  88.0   96.0   96.0   91.0   \n",
       "3920        6.992000  2022   82.0  75.0   92.0  81.0  100.0   93.0   93.0   \n",
       "3921        7.601504  2022   89.0  94.0   97.0  90.0   98.0   94.0   93.0   \n",
       "\n",
       "         Q8  ...  Q9_lag_1  Q10_lag_1  Q11_lag_1  Q12_lag_1  Q13_lag_1  \\\n",
       "1      95.0  ...      98.0       96.0       96.0       96.0       96.0   \n",
       "2      98.0  ...      92.0       92.0       89.0       97.0       95.0   \n",
       "3      96.0  ...      96.0       97.0       96.0       98.0       98.0   \n",
       "4      90.0  ...      98.0       97.0       99.0       97.0       99.0   \n",
       "5      97.0  ...      91.0       90.0       90.0       89.0       90.0   \n",
       "...     ...  ...       ...        ...        ...        ...        ...   \n",
       "3917  100.0  ...      90.0       88.0       88.0       94.0       91.0   \n",
       "3918   93.0  ...     100.0       99.0       99.0      100.0       99.0   \n",
       "3919   92.0  ...      98.0       94.0       92.0       91.0       93.0   \n",
       "3920   93.0  ...      94.0       87.0       97.0       89.0       89.0   \n",
       "3921   94.0  ...      85.0       89.0      100.0      100.0      100.0   \n",
       "\n",
       "      Q14_lag_1  Q15_lag_1  Q16_lag_1  Q17_lag_1  Q18_lag_1  \n",
       "1          98.0       94.0       94.0       96.0       89.0  \n",
       "2          91.0       94.0       95.0       94.0       88.0  \n",
       "3          98.0       98.0       96.0       98.0       89.0  \n",
       "4          99.0       99.0       96.0       97.0       91.0  \n",
       "5          94.0       92.0       91.0       96.0       90.0  \n",
       "...         ...        ...        ...        ...        ...  \n",
       "3917       97.0       95.0       93.0       95.0       21.0  \n",
       "3918      100.0       99.0       99.0       99.0        8.0  \n",
       "3919       92.0       96.0       98.0       98.0        9.0  \n",
       "3920       92.0       94.0       89.0       93.0       26.0  \n",
       "3921       82.0      100.0       69.0       92.0       56.0  \n",
       "\n",
       "[3921 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy of data with lagged features\n",
    "\n",
    "lag = 1  \n",
    "df_lag = df.copy()\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in ['Year', 'Pavement Rating']:\n",
    "        df_lag[f'{col}_lag_{lag}'] = df_lag[col].shift(lag)\n",
    "\n",
    "df_lag.dropna() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_rating</th>\n",
       "      <th>year</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Q4</th>\n",
       "      <th>Q5</th>\n",
       "      <th>Q6</th>\n",
       "      <th>Q7</th>\n",
       "      <th>Q8</th>\n",
       "      <th>...</th>\n",
       "      <th>Q14_rolling_mean_3</th>\n",
       "      <th>Q14_rolling_std_3</th>\n",
       "      <th>Q15_rolling_mean_3</th>\n",
       "      <th>Q15_rolling_std_3</th>\n",
       "      <th>Q16_rolling_mean_3</th>\n",
       "      <th>Q16_rolling_std_3</th>\n",
       "      <th>Q17_rolling_mean_3</th>\n",
       "      <th>Q17_rolling_std_3</th>\n",
       "      <th>Q18_rolling_mean_3</th>\n",
       "      <th>Q18_rolling_std_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>92.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.666667</td>\n",
       "      <td>4.041452</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>2.309401</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>88.666667</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>2017</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>4.358899</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>95.666667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>96.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>89.333333</td>\n",
       "      <td>1.527525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>2017</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>...</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>96.333333</td>\n",
       "      <td>3.785939</td>\n",
       "      <td>94.333333</td>\n",
       "      <td>2.886751</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>2017</td>\n",
       "      <td>93.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>2.516611</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>94.666667</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.428571</td>\n",
       "      <td>2017</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>93.666667</td>\n",
       "      <td>2.886751</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>95.666667</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>92.333333</td>\n",
       "      <td>2.516611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>7.375000</td>\n",
       "      <td>2022</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>97.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>97.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>19.666667</td>\n",
       "      <td>11.060440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>7.478261</td>\n",
       "      <td>2022</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.333333</td>\n",
       "      <td>4.041452</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>97.333333</td>\n",
       "      <td>2.081666</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>7.234178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>6.967742</td>\n",
       "      <td>2022</td>\n",
       "      <td>86.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.666667</td>\n",
       "      <td>4.618802</td>\n",
       "      <td>96.333333</td>\n",
       "      <td>2.516611</td>\n",
       "      <td>95.333333</td>\n",
       "      <td>5.507571</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>14.333333</td>\n",
       "      <td>10.115994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3920</th>\n",
       "      <td>6.992000</td>\n",
       "      <td>2022</td>\n",
       "      <td>82.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>88.666667</td>\n",
       "      <td>5.773503</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>3.055050</td>\n",
       "      <td>85.333333</td>\n",
       "      <td>14.843629</td>\n",
       "      <td>94.333333</td>\n",
       "      <td>3.214550</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>23.797759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>7.601504</td>\n",
       "      <td>2022</td>\n",
       "      <td>89.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>7.211103</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>3.055050</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>13.228757</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>1.527525</td>\n",
       "      <td>34.333333</td>\n",
       "      <td>18.929694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3920 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      average_rating  year     Q1    Q2     Q3    Q4     Q5     Q6     Q7  \\\n",
       "2           6.000000  2017   92.0  96.0   99.0  98.0   97.0   98.0   99.0   \n",
       "3           6.800000  2017   98.0  99.0   98.0  98.0   98.0   98.0   98.0   \n",
       "4           7.000000  2017   90.0  90.0   95.0  91.0   95.0   94.0   89.0   \n",
       "5           7.400000  2017   93.0  96.0   98.0  95.0   98.0   97.0   97.0   \n",
       "6           8.428571  2017   86.0  88.0   96.0  91.0   93.0   90.0   90.0   \n",
       "...              ...   ...    ...   ...    ...   ...    ...    ...    ...   \n",
       "3917        7.375000  2022  100.0  99.0  100.0  99.0  100.0  100.0  100.0   \n",
       "3918        7.478261  2022   96.0  94.0   98.0  93.0   98.0   93.0   93.0   \n",
       "3919        6.967742  2022   86.0  91.0   98.0  88.0   96.0   96.0   91.0   \n",
       "3920        6.992000  2022   82.0  75.0   92.0  81.0  100.0   93.0   93.0   \n",
       "3921        7.601504  2022   89.0  94.0   97.0  90.0   98.0   94.0   93.0   \n",
       "\n",
       "         Q8  ...  Q14_rolling_mean_3  Q14_rolling_std_3  Q15_rolling_mean_3  \\\n",
       "2      98.0  ...           95.666667           4.041452           95.333333   \n",
       "3      96.0  ...           96.000000           4.358899           97.000000   \n",
       "4      90.0  ...           97.000000           2.645751           96.333333   \n",
       "5      97.0  ...           96.666667           2.516611           96.000000   \n",
       "6      89.0  ...           96.000000           1.732051           93.666667   \n",
       "...     ...  ...                 ...                ...                 ...   \n",
       "3917  100.0  ...           97.000000           3.000000           97.333333   \n",
       "3918   93.0  ...           96.333333           4.041452           96.666667   \n",
       "3919   92.0  ...           94.666667           4.618802           96.333333   \n",
       "3920   93.0  ...           88.666667           5.773503           96.666667   \n",
       "3921   94.0  ...           90.000000           7.211103           96.666667   \n",
       "\n",
       "      Q15_rolling_std_3  Q16_rolling_mean_3  Q16_rolling_std_3  \\\n",
       "2              2.309401           95.000000           1.000000   \n",
       "3              2.645751           95.666667           0.577350   \n",
       "4              3.785939           94.333333           2.886751   \n",
       "5              3.605551           94.666667           3.214550   \n",
       "6              2.886751           93.333333           3.214550   \n",
       "...                 ...                 ...                ...   \n",
       "3917           2.081666           96.666667           3.214550   \n",
       "3918           2.081666           96.666667           3.214550   \n",
       "3919           2.516611           95.333333           5.507571   \n",
       "3920           3.055050           85.333333          14.843629   \n",
       "3921           3.055050           84.000000          13.228757   \n",
       "\n",
       "      Q17_rolling_mean_3  Q17_rolling_std_3  Q18_rolling_mean_3  \\\n",
       "2              96.000000           2.000000           88.666667   \n",
       "3              96.333333           2.081666           89.333333   \n",
       "4              97.000000           1.000000           90.000000   \n",
       "5              96.666667           0.577350           91.000000   \n",
       "6              95.666667           1.527525           92.333333   \n",
       "...                  ...                ...                 ...   \n",
       "3917           97.333333           2.081666           19.666667   \n",
       "3918           97.333333           2.081666           12.666667   \n",
       "3919           96.666667           3.214550           14.333333   \n",
       "3920           94.333333           3.214550           30.333333   \n",
       "3921           93.333333           1.527525           34.333333   \n",
       "\n",
       "      Q18_rolling_std_3  \n",
       "2              0.577350  \n",
       "3              1.527525  \n",
       "4              1.000000  \n",
       "5              1.000000  \n",
       "6              2.516611  \n",
       "...                 ...  \n",
       "3917          11.060440  \n",
       "3918           7.234178  \n",
       "3919          10.115994  \n",
       "3920          23.797759  \n",
       "3921          18.929694  \n",
       "\n",
       "[3920 rows x 60 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy of data with rolling avgs\n",
    "\n",
    "window = 3  \n",
    "df_rolling = df.copy()\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in ['Year', 'Pavement Rating']:\n",
    "        df_rolling[f'{col}_rolling_mean_{window}'] = df_rolling[col].rolling(window=window).mean()\n",
    "        df_rolling[f'{col}_rolling_std_{window}'] = df_rolling[col].rolling(window=window).std()\n",
    "\n",
    "df_rolling.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "preprocessor = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  \n",
    "    (\"scaler\", RobustScaler()) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (df) - MAE: 0.3834 (±0.1017), R²: -0.5902 (±0.8413)\n",
      "Lag Data (df_lag) - MAE: 0.3721 (±0.0992), R²: -0.5092 (±0.7124)\n",
      "Rolling Data (df_rolling) - MAE: 67776056201.5699 (±135552112402.4827), R²: -604409889282383402762240.0000 (±1208819778564766805524480.0000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "y_df = df['average_rating']  \n",
    "X_df = df.drop(columns=['average_rating'])\n",
    "X_df_lag = df_lag.drop(columns=['average_rating'])\n",
    "y_lag = df_lag['average_rating']  \n",
    "X_df_rolling = df_rolling.drop(columns=['average_rating'])\n",
    "y_rolling = df_rolling['average_rating']  \n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "def evaluate_model(X, y, model, tscv, preprocessor):\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    mae_scores = cross_val_score(pipeline, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "    r2_scores = cross_val_score(pipeline, X, y, cv=tscv, scoring='r2')\n",
    "    \n",
    "    return mae_scores, r2_scores\n",
    "\n",
    "mae_original, r2_original = evaluate_model(X_df, y_df, lr_model, tscv, preprocessor)\n",
    "mae_lag, r2_lag = evaluate_model(X_df_lag, y_lag, lr_model, tscv, preprocessor)\n",
    "mae_rolling, r2_rolling = evaluate_model(X_df_rolling, y_rolling, lr_model, tscv, preprocessor)\n",
    "\n",
    "print(f\"Original Data (df) - MAE: {-mae_original.mean():.4f} (±{mae_original.std():.4f}), R²: {r2_original.mean():.4f} (±{r2_original.std():.4f})\")\n",
    "print(f\"Lag Data (df_lag) - MAE: {-mae_lag.mean():.4f} (±{mae_lag.std():.4f}), R²: {r2_lag.mean():.4f} (±{r2_lag.std():.4f})\")\n",
    "print(f\"Rolling Data (df_rolling) - MAE: {-mae_rolling.mean():.4f} (±{mae_rolling.std():.4f}), R²: {r2_rolling.mean():.4f} (±{r2_rolling.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, we will use our data with lagged features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_df_lag\n",
    "y = y_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesSplit MAE: 0.3371 (±0.1085)\n",
      "TimeSeriesSplit R²: -0.1179 (±0.0936)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', rf_model)\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "mae_scores_ts = cross_val_score(rf_pipeline, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "r2_scores_ts = cross_val_score(rf_pipeline, X, y, cv=tscv, scoring='r2')\n",
    "\n",
    "print(f\"TimeSeriesSplit MAE: {-mae_scores_ts.mean():.4f} (±{mae_scores_ts.std():.4f})\")\n",
    "print(f\"TimeSeriesSplit R²: {r2_scores_ts.mean():.4f} (±{r2_scores_ts.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from xgboost) (1.14.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost TimeSeriesSplit MAE: 0.4096 (±0.0900)\n",
      "XGBoost TimeSeriesSplit R²: -0.7692 (±0.6625)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb_model)\n",
    "])\n",
    "\n",
    "mae_scores_xgb = cross_val_score(xgb_pipeline, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "r2_scores_xgb = cross_val_score(xgb_pipeline, X, y, cv=tscv, scoring='r2')\n",
    "\n",
    "print(f\"XGBoost TimeSeriesSplit MAE: {-mae_scores_xgb.mean():.4f} (±{mae_scores_xgb.std():.4f})\")\n",
    "print(f\"XGBoost TimeSeriesSplit R²: {r2_scores_xgb.mean():.4f} (±{r2_scores_xgb.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR TimeSeriesSplit MAE: 0.3686 (±0.0879)\n",
      "SVR TimeSeriesSplit R²: -0.4272 (±0.3320)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_model = SVR()\n",
    "\n",
    "svr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', svr_model)\n",
    "])\n",
    "\n",
    "mae_scores_svr = cross_val_score(svr_pipeline, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
    "r2_scores_svr = cross_val_score(svr_pipeline, X, y, cv=tscv, scoring='r2')\n",
    "\n",
    "print(f\"SVR TimeSeriesSplit MAE: {-mae_scores_svr.mean():.4f} (±{mae_scores_svr.std():.4f})\")\n",
    "print(f\"SVR TimeSeriesSplit R²: {r2_scores_svr.mean():.4f} (±{r2_scores_svr.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag_clean = df_lag.dropna(subset=[col for col in df_lag.columns if 'lag' in col])\n",
    "\n",
    "X_clean = df_lag_clean.drop(columns=['average_rating'])\n",
    "y_clean = df_lag_clean['average_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 22:52:09.684383: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 16.2009\n",
      "Epoch 2/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6676\n",
      "Epoch 3/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6422\n",
      "Epoch 4/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5793\n",
      "Epoch 5/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5962\n",
      "Epoch 6/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6023 \n",
      "Epoch 7/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5558\n",
      "Epoch 8/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5499\n",
      "Epoch 9/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5340\n",
      "Epoch 10/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5475\n",
      "Epoch 11/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5303\n",
      "Epoch 12/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5399\n",
      "Epoch 13/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5511\n",
      "Epoch 14/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5259\n",
      "Epoch 15/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5208\n",
      "Epoch 16/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5413\n",
      "Epoch 17/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5158\n",
      "Epoch 18/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5161\n",
      "Epoch 19/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5242\n",
      "Epoch 20/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5631\n",
      "Epoch 21/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5087\n",
      "Epoch 22/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5363\n",
      "Epoch 23/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5169 \n",
      "Epoch 24/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5117\n",
      "Epoch 25/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5168\n",
      "Epoch 26/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5212\n",
      "Epoch 27/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5232\n",
      "Epoch 28/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5312\n",
      "Epoch 29/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5449\n",
      "Epoch 30/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5114\n",
      "Epoch 31/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5170\n",
      "Epoch 32/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5456\n",
      "Epoch 33/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5048\n",
      "Epoch 34/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4847\n",
      "Epoch 35/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4836\n",
      "Epoch 36/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5174\n",
      "Epoch 37/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5205\n",
      "Epoch 38/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5262\n",
      "Epoch 39/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4894\n",
      "Epoch 40/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5016\n",
      "Epoch 41/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4972\n",
      "Epoch 42/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5050 \n",
      "Epoch 43/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4988\n",
      "Epoch 44/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4730 \n",
      "Epoch 45/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5107\n",
      "Epoch 46/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5158\n",
      "Epoch 47/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4759\n",
      "Epoch 48/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5119\n",
      "Epoch 49/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5133\n",
      "Epoch 50/50\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4857\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "MAE: 0.3126\n",
      "R²: -0.0140\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "def create_sequences(data, target, time_steps=1):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X_seq.append(data[i:i+time_steps])\n",
    "        y_seq.append(target[i+time_steps])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "time_steps = 3 \n",
    "X_seq, y_seq = create_sequences(X_scaled, y_clean, time_steps)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=False, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1)) \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "model = create_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras-tuner) (3.6.0)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from keras-tuner) (24.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras->keras-tuner) (2.0.2)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras->keras-tuner) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras->keras-tuner) (3.12.1)\n",
      "Requirement already satisfied: optree in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras->keras-tuner) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/python/3.12.1/lib/python3.12/site-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->keras-tuner) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->keras-tuner) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from optree->keras->keras-tuner) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->keras->keras-tuner) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/lstm_tuning/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    for i in range(hp.Int('num_lstm_layers', 1, 2)):  \n",
    "        model.add(\n",
    "            LSTM(\n",
    "                units=hp.Int('lstm_units', min_value=50, max_value=150, step=50),  \n",
    "                return_sequences=(i < hp.get('num_lstm_layers') - 1), \n",
    "                input_shape=(X_train.shape[1], X_train.shape[2]) if i == 0 else None\n",
    "            )\n",
    "        )\n",
    "        model.add(Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))  \n",
    "\n",
    "    model.add(Dense(units=1))  \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=0.0001, max_value=0.001, step=0.0001)),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',  \n",
    "    factor=3,  \n",
    "    directory='my_dir',  \n",
    "    project_name='lstm_tuning'\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.1295 \n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "Best model loss: 0.1772\n",
      "MAE: 0.3130\n",
      "R²: -0.0003\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "loss = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best model loss: {loss:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_lstm_layers': 2, 'lstm_units': 50, 'dropout_rate': 0.4, 'learning_rate': 0.0009000000000000001, 'tuner/epochs': 50, 'tuner/initial_epoch': 17, 'tuner/bracket': 3, 'tuner/round': 3, 'tuner/trial_id': '0046'}\n"
     ]
    }
   ],
   "source": [
    "best_hp = tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters\n",
    "print(\"Best hyperparameters: \", best_hp.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of LSTM layers:** 2\n",
    "- **LSTM units:** 50\n",
    "- **Dropout rate:** 0.4\n",
    "- **Learning rate:** 0.0009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.25489566821831827\n",
      "R^2 Score: 0.03281009208827801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "X = df.drop(columns=['average_rating']) \n",
    "y = df['average_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Importance\n",
      "18     Q18    0.090443\n",
      "0     year    0.080264\n",
      "2       Q2    0.058122\n",
      "16     Q16    0.057274\n",
      "1       Q1    0.054458\n",
      "14     Q14    0.053787\n",
      "10     Q10    0.052869\n",
      "11     Q11    0.051489\n",
      "13     Q13    0.051027\n",
      "4       Q4    0.050787\n",
      "15     Q15    0.048265\n",
      "9       Q9    0.047859\n",
      "5       Q5    0.047581\n",
      "17     Q17    0.046331\n",
      "12     Q12    0.043671\n",
      "8       Q8    0.043541\n",
      "6       Q6    0.041215\n",
      "3       Q3    0.040904\n",
      "7       Q7    0.040115\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feature_importances = rf_regressor.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
